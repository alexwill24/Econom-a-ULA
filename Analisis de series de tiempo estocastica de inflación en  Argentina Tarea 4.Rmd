---
title: "SERIES DE TIEMPO ARIMA"
author: "William A. Gutierrez V."
date: "`r Sys.Date()`"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Analisis de series de tiempo

El motivo de la introducción de los modelos ARIMA nace del hecho de que no se puede trabajar con una serie temporal no estacionaria. Se dice que una serie es estacionaria cuando su media, varianza y autocovarianza son invariantes en el tiempo. La mayoría de series temporales económicas no son estacionarias pero diferenciándolas un número determinado de veces la serie original se transforma en estacionaria, con lo cual ya se podría aplicar la metodología de los modelos ARIMA. A continuacion se mostrara la serie de tiempo **"Inflacion de argentina"** en la cual se mostrara la sucesiones de datos mensuales desde el año 1970 hasta la actualidad.

[MODELO GENERAL ARIMA(p,q,d)](images/descarga.jpeg){width="318"}

A continuacion se buscara el modelo que mejor se ajuste a la serie de datos.

**Introducción de los datos**

```{r}
library(ggplot2)
library(forecast)
library(tseries)
library(readxl)
library(lmtest)
datos <- read_excel("C:/Users/MARK/Desktop/Econometria II/datos de series de tiempo argentina.xlsx")


datos.ts= ts(datos)



```

**Trazamos la serie de tiempo con ggplot**

```{r}
ggplot() +
  geom_line(data = datos, aes(x = Fecha , y = Valor)) + ylab('Inflacion de argentina')


```

```{r}

#usamos medias moviles para suavizar el componente estacional

datos$Valor.ma = ma(datos$Valor
                    , order=7) 
datos$Valor30.ma = ma(datos$Valor, order=30)


ggplot() +
  geom_line(data = datos, aes(x = Fecha, y = Valor, colour = "serie original")) +
  geom_line(data = datos, aes(x = Fecha, y = Valor.ma,   colour = "Weekly Moving Average"))  +
  geom_line(data = datos, aes(x = Fecha, y = Valor30.ma, colour = "Monthly Moving Average"))  +
  ylab('Inflacion de argentina')
```

Primero, calculamos el componente estacional del uso de datos stl(). A continuación,hallamos el componente estacional de la serie mediante suavizado y ajusta la serie original restando la estacionalidad.

```{r}
count_ma = ts(na.omit(datos$Valor30.ma), frequency=30)
decomp = stl(count_ma, s.window="periodic")
deseasonal_inf <- seasadj(decomp)
plot(decomp)
#  Los datos están en la segunda columna:
datos <- datos[[2]]

# Convertir los datos a un objeto de serie de tiempo
datos.ts <- ts(datos, start = c(1970, 7), frequency = 12)

# Verifica la longitud de la serie de tiempo
length(datos.ts)

# Descomponer la serie de tiempo
datos_decomp <- decompose(datos.ts)

# Dividir la ventana gráfica en una matriz de 2 filas y 2 columnas
par(mfrow = c(2, 2))

# Graficar los componentes descompuestos de la serie de tiempo
plot(datos_decomp$x, main = "Serie de tiempo - Original", col = "black", ylab = "Valores")
plot(datos_decomp$trend, main = "Tendencia", col = "blue", ylab = "Valores")
plot(datos_decomp$seasonal, main = "Estacionalidad", col = "red", ylab = "Valores")
plot(datos_decomp$random, main = "Irregularidad", col = "green", ylab = "Valores")
```

## Estacionariedad

La instalación de un modelo ARIMA requiere que la serie sea estacionaria . Se dice que una serie es estacionaria cuando su media, varianza y autocovarianza son invariantes en el tiempo.

Esta suposición tiene un sentido intuitivo:

Dado que ARIMA usa retardos previos de series para modelar su comportamiento

```{r}
adf.test(count_ma, alternative = "stationary")

```

Contraste de hipótesis:

H0: No estacionaria H1: Estacionaria

Tras realizar la prueba aumentada de Dickey-Fuller (ADF), obtenemos un p-valor = 0.02 Como el p-valor \< 0.05, rechazamos H0. Podemos concluir que nuestra serie temporal es estacionaria.

**Autocorrelacion simple**

Las ACF proporcionan información sobre cómo una observación influye en las siguientes. Las PACF proporcionan la relación directa existente entre observaciones separadas por k retardos.

```{r}
Acf(count_ma, main='')

Pacf(count_ma, main='')
```

**Serie de tiempo diferenciadas, es**

```{r}
#serie estacionaria
count_d1 = diff(deseasonal_inf, differences = 1)
plot(count_d1)

#decomposicion
decomp2 = stl(count_d1, s.window="periodic")
deseasonal_inf2 <- seasadj(decomp2)
plot(decomp2)
```

estacionariedad

```{r}
adf.test(count_d1, alternative = "stationary")
```

**AUTOCORRELACION DE LA SERIE DIFERENCIADA**

```{r}
Acf(count_d1, main='')

Pacf(count_d1, main='')
```

**Modelo arima**

```{r}
#serie sin diferenciar
auto.arima(deseasonal_inf, seasonal=FALSE)

#serie diferenciar
auto.arima(deseasonal_inf2, seasonal=FALSE)

```

Por lo tanto, basados en los criterios de informacion, correlogramas y ademas por parsimonia, elegimos la serie diferenciada.

```{r}
library(TSA)
modeloarima<-auto.arima(deseasonal_inf, seasonal=FALSE)


##Podemos especificar el horizonte de pronóstico h periodos por delante para que se realicen las predicciones, y usar el modelo ajustado para generar dichas predicciones:

#prediccion para los proximos treinta meses con una confianza del 95%
prediccion <- forecast(modeloarima, h=100, level= c(95))
plot(prediccion)



# Calcular las medidas de precisión del modelo
accuracy_measures <- accuracy(modeloarima)

# Imprimir las medidas de precisión
print(accuracy_measures)
```

#vALIDACIÓN#
```{r}

# Prueba de Ljung-Box
Box.test(datos.ts, lag = 20, type = "Ljung-Box")


# Prueba de Shapiro-Wilk
shapiro.test(datos_decomp$random)
```

### Resumen de la Interpretación

-   **Error Medio (ME)**: Muy cercano a 0, lo cual es positivo y sugiere que el modelo no está sesgado.

-   **RMSE y MAE**: Relativamente bajos, lo cual indica que el modelo se ajusta bien a los datos.

-   **MPE**: Bajo, indicando un buen rendimiento en términos de error porcentual medio.

-   **MAPE**: Alto, sugiriendo posibles problemas con valores atípicos o un ajuste deficiente en ciertas partes de los datos.

-   **MASE**: Menor que 1, indicando que el modelo es mejor que un modelo naive.

-   **ACF1**: Muy cercano a 0, indicando que no hay autocorrelación significativa en los errores residuales.
